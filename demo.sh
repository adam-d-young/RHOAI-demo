#!/bin/bash

######################################################################
# RHOAI Demo Script
# Uses demo-magic for stepped command execution with commentary.
# Press ENTER to advance each step. Ctrl+C to exit.
#
# Prerequisites (run setup.sh first):
#   - oc CLI logged into target cluster
#   - NFD Operator installed
#   - NVIDIA GPU Operator installed with ClusterPolicy
#   - GPU machineset provisioned (A10G x2)
#   - MinIO deployed (S3 storage)
#   - MySQL deployed (Model Registry backend)
######################################################################

DEMO_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Preflight: check required tools
MISSING=""
for tool in oc bat; do
  if ! command -v "$tool" &>/dev/null; then
    MISSING="$MISSING $tool"
  fi
done
if ! oc whoami &>/dev/null; then
  MISSING="$MISSING oc(not-logged-in)"
fi
if [ -n "$MISSING" ]; then
  echo "ERROR: Missing required tools:$MISSING"
  echo ""
  echo "  oc   â†’ brew install openshift-cli (or download from OpenShift console)"
  echo "  bat  â†’ brew install bat"
  echo "  oc login â†’ oc login <cluster-url>"
  exit 1
fi

# Source demo-magic
. "${DEMO_DIR}/demo-magic.sh" -n

# Helper: verify a condition before continuing, retry on failure
# Usage: verify_step "description" "command that returns 0 on success"
verify_step() {
  local desc="$1"
  local cmd="$2"
  while true; do
    if eval "$cmd" &>/dev/null; then
      echo -e "  ${GREEN}âœ… ${desc}${COLOR_RESET}"
      return 0
    else
      echo -e "  ${RED}âŒ ${desc} -- not ready${COLOR_RESET}"
      echo ""
      read -p "  Press ENTER to retry, or 's' to skip: " choice
      if [ "$choice" = "s" ]; then
        echo -e "  ${CYAN}â­ï¸  Skipped${COLOR_RESET}"
        return 1
      fi
    fi
  done
}

# Helper: compare live resource against manifest, show diff if mismatched
# Usage: verify_manifest "description" "manifest-file"
verify_manifest() {
  local desc="$1"
  local manifest="$2"
  while true; do
    local diff_output
    diff_output=$(oc diff -f "${DEMO_DIR}/${manifest}" 2>&1)
    local rc=$?
    if [ $rc -eq 0 ]; then
      echo -e "  ${GREEN}âœ… ${desc} -- matches manifest${COLOR_RESET}"
      return 0
    elif [ $rc -eq 1 ]; then
      echo -e "  ${CYAN}âš ï¸  ${desc} -- differs from manifest:${COLOR_RESET}"
      echo ""
      echo "$diff_output"
      echo ""
      read -p "  (a)pply manifest to fix / (c)ontinue anyway / (r)etry check: " choice
      case "$choice" in
        a) oc apply -f "${DEMO_DIR}/${manifest}" &>/dev/null
           echo -e "  ${GREEN}  Applied.${COLOR_RESET}"
           continue ;;
        c) return 0 ;;
        *) continue ;;
      esac
    else
      echo -e "  ${RED}âŒ ${desc} -- not found or error${COLOR_RESET}"
      echo ""
      read -p "  Press ENTER to retry, or 's' to skip: " choice
      if [ "$choice" = "s" ]; then
        echo -e "  ${CYAN}â­ï¸  Skipped${COLOR_RESET}"
        return 1
      fi
    fi
  done
}

# Helper: section header with optional skip
# Returns 1 if skipped (use: begin_section ... || return 0)
begin_section() {
  local num="$1" icon="$2" title="$3"
  echo ""
  echo -e "# ${icon} ${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}"
  echo -e "# ${GREEN}SECTION ${num}: ${title}${COLOR_RESET}"
  echo -e "# ${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}"
  read -p "  Skip this section? (y/N): " SKIP_SECTION
  if [ "$SKIP_SECTION" = "y" ]; then
    echo -e "  ${CYAN}â­ï¸  Skipped Section ${num}${COLOR_RESET}"
    return 1
  fi
  return 0
}

# Helper: ensure a variable is set, try to resolve it if not
# Usage: ensure_var RHOAI_URL "oc get gateway ..."
ensure_var() {
  local varname="$1"
  local cmd="$2"
  if [ -z "${!varname:-}" ]; then
    local val
    val=$(eval "$cmd" 2>/dev/null) || true
    if [ -n "$val" ]; then
      eval "$varname=\"$val\""
      echo -e "  ${GREEN}âœ… ${varname} resolved${COLOR_RESET}"
    else
      echo -e "  ${RED}âš ï¸  Could not resolve ${varname} -- was a previous section skipped?${COLOR_RESET}"
    fi
  fi
}

# Configure
DEMO_PROMPT="${GREEN}âœ ${CYAN}\W ${COLOR_RESET}"
TYPE_SPEED=20

# Detect browser-open command (macOS vs Linux)
if command -v open &>/dev/null; then
  BROWSER_OPEN="open"
elif command -v xdg-open &>/dev/null; then
  BROWSER_OPEN="xdg-open"
else
  BROWSER_OPEN="echo"   # fallback: just print the URL
fi

clear

echo ""
echo -e "${GREEN}  ___                 ___ _    _  __ _       _   ___ ${COLOR_RESET}"
echo -e "${GREEN} / _ \ _ __  ___ _ _ / __| |_ (_)/ _| |_    /_\ |_ _|${COLOR_RESET}"
echo -e "${GREEN}| (_) | '_ \/ -_) ' \\\\__ \ ' \| |_|  _|     / _ \ | | ${COLOR_RESET}"
echo -e "${GREEN} \___/| .__/\___|_||_|___/_||_|_|_|  \__| /_/ \_\___|${COLOR_RESET}"
echo -e "${GREEN}      |_|${COLOR_RESET}         ${CYAN}Get Started with OpenShift AI${COLOR_RESET}"
echo ""
echo -e "  ${CYAN}FSI Bootcamp Demo  â€¢  GPU-Accelerated ML on OpenShift${COLOR_RESET}"
echo ""

wait

######################################################################
# Section functions
######################################################################

section_1() {
begin_section 1 "ğŸ”" "Check Current State" || return 0
echo "#"
echo "# ğŸ’° GPUs are the most expensive resource in your cluster."
echo "#   Without proper scheduling and isolation, teams fight over them"
echo "#   or they sit idle. We need a stack that:"
echo "#   â€¢ Discovers GPU hardware automatically (NFD)"
echo "#   â€¢ Manages drivers, device plugins, and monitoring (GPU Operator)"
echo "#   â€¢ Controls who can schedule on GPU nodes (taints + tolerations)"

wait

pe "oc get csv -A | grep -E 'nvidia|nfd|rhods'"

echo ""
echo "# ğŸ–¥ï¸  GPU nodes online?"

wait

pe "oc get nodes -l nvidia.com/gpu.present=true"

pe "oc get nodes -l nvidia.com/gpu.present=true -o custom-columns='NODE:.metadata.name,TAINT:.spec.taints[*].key,EFFECT:.spec.taints[*].effect'"

echo ""
echo "# ğŸš« These GPU nodes are tainted: nvidia.com/gpu=NoSchedule"
echo "#   â€¢ Without this, ANY pod could land on a \$3/hr GPU node"
echo "#   â€¢ Taints block everything by default -- only approved workloads get in"
echo "#   â€¢ GPU Operator pods tolerate it (they manage the hardware)"
echo "#   â€¢ We'll create a HardwareProfile later to let ML workloads through"

wait
}

section_2() {
begin_section 2 "ğŸ”" "Node Feature Discovery (NFD)" || return 0
echo "#"
echo "# ğŸ‘ï¸  NFD = the eyes of the cluster"
echo "#   â€¢ DaemonSet on every node -- scans for hardware"
echo "#   â€¢ GPUs, FPGAs, SR-IOV -- auto-labeled on the node"
echo "#   â€¢ GPU Operator reads these labels to deploy drivers"

wait

pe "oc get nodefeaturediscovery -n openshift-nfd"

echo ""
echo "# ğŸ·ï¸  What did NFD find on our GPU nodes?"
echo "#   NFD labels use prefix: feature.node.kubernetes.io/"
echo "#   Key ones:"
echo "#     â€¢ pci-10de.present=true  â†’ 10de = NVIDIA's PCI vendor ID"
echo "#     â€¢ kernel.version         â†’ running kernel"
echo "#     â€¢ system-os_release.ID   â†’ RHCOS / RHEL"

wait

pe "oc describe node \$(oc get nodes -l nvidia.com/gpu.present=true -o jsonpath='{.items[0].metadata.name}') | grep -E 'pci-10de|kernel-version.full|os_release.ID|cpu-model.vendor'"

wait
}

section_3() {
begin_section 3 "ğŸ®" "NVIDIA GPU Operator" || return 0
echo "#"
echo "# ğŸ”§ One operator, entire GPU stack:"
echo "#   â€¢ Drivers, device plugins, container toolkit, monitoring"
echo "#   â€¢ All driven by a single CR: ClusterPolicy"

wait

pe "oc get clusterpolicy"

echo ""
echo "# ğŸ“‹ What does the ClusterPolicy configure?"

wait

pe "bat --style=grid,numbers manifests/gpu-cluster-policy.yaml"

echo ""
echo ""
echo "# ğŸ·ï¸  GPU Feature Discovery (GFD) adds nvidia.com/gpu.* labels"
echo "#   â€¢ Product name, VRAM, CUDA version, driver version"
echo "#   â€¢ Different from NFD -- GFD queries the GPU directly"

wait

pe "oc get nodes -l nvidia.com/gpu.present=true -o custom-columns='NODE:.metadata.name,GPU:.metadata.labels.nvidia\.com/gpu\.product,VRAM_MB:.metadata.labels.nvidia\.com/gpu\.memory,GPUs:.status.allocatable.nvidia\.com/gpu'"

echo ""
echo "# ğŸš€ Moment of truth -- nvidia-smi"
echo "#   â€¢ NVIDIA System Management Interface -- CLI to query the GPU"
echo "#   â€¢ We're running it FROM INSIDE a driver pod (not the host)"
echo "#   â€¢ If it returns output, the full stack is working:"
echo "#     drivers compiled â†’ device plugin registered â†’ toolkit configured"
echo "#"
echo "# ğŸ“– How to read the output:"
echo "#   â€¢ GPU name + VRAM (A10G, 23028MiB â‰ˆ 22.5GiB -- marketed as 24GB)"
echo "#   â€¢ Driver 570.x + CUDA 12.8"
echo "#   â€¢ Pwr: 24W/300W â†’ idle draw / max cap (300W under full load)"
echo "#   â€¢ P8 = performance state (P0=max, P12=min) -- P8 means idle"
echo "#   â€¢ Temp 28C â†’ cool, expect 60-80C under load"
echo "#   â€¢ GPU-Util 0%, no processes â†’ nothing scheduled yet"

wait

pe "oc exec -n nvidia-gpu-operator \$(oc get pods -n nvidia-gpu-operator --no-headers | grep driver | awk '{print \$1}' | head -n 1) -c nvidia-driver-ctr -- nvidia-smi"

wait
}

section_4() {
begin_section 4 "ğŸ“¦" "Install Red Hat OpenShift AI" || return 0
echo "#"
echo "# ğŸ§  Problem: OpenShift gives you containers and GPUs, but"
echo "#   data scientists still need notebooks, model serving, pipelines,"
echo "#   a model registry, and a catalog of foundation models."
echo "#   Building all that from scratch is months of work."
echo "#"
echo "# ğŸ“¦ RHOAI = the ML platform layer that turns OpenShift into"
echo "#   a self-service AI development environment. One operator install."
echo "#   Everything you'll use in this bootcamp -- workbenches, LlamaStack,"
echo "#   pipelines, model serving -- runs on top of this."

wait

OCP_CONSOLE=$(oc whoami --show-console)
$BROWSER_OPEN $OCP_CONSOLE

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Install RHOAI in browser${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ OpenShift Console â†’ Operators â†’ OperatorHub"
echo "#   â†’ Search: 'OpenShift AI'"
echo "#   â†’ Click 'Red Hat OpenShift AI'"
echo "#   â†’ Click 'Install'"
echo "#   â†’ Channel: fast | Update approval: Automatic"
echo "#   â†’ Accept all other defaults â†’ Click 'Install'"
echo "#   â†’ â³ Wait for CSV status: 'Succeeded'"
echo "#"
echo -e "# ${RED}   DO NOT press ENTER until the operator shows 'Succeeded'${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”„ Verify the operator installed:"

wait

pe "oc get csv -n redhat-ods-operator | grep rhods"

verify_step "RHOAI operator CSV is Succeeded" "oc get csv -n redhat-ods-operator 2>/dev/null | grep rhods | grep -q Succeeded"

echo ""
echo "# ğŸ§© The operator is installed, but it doesn't DO anything yet."
echo "#   We need a DataScienceCluster (DSC) -- the CR that tells"
echo "#   the operator which components to activate."
echo "#"
echo "# ğŸ“‹ DSC components we need:"
echo "#   â€¢ Dashboard, Workbenches, ModelRegistry â†’ Managed (defaults)"
echo "#   â€¢ KServe â†’ Managed (model serving)"
echo "#   â€¢ DataSciencePipelines â†’ Managed (ML pipelines)"
echo "#   â€¢ ModelMeshServing â†’ Removed (deprecated, KServe replaces it)"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create DataScienceCluster${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ OpenShift Console â†’ Installed Operators â†’ Red Hat OpenShift AI"
echo "#   â†’ 'DataScienceCluster' tab â†’ Click 'Create DataScienceCluster'"
echo "#"
echo "# ğŸ’¡ All defaults are fine (Dashboard, KServe, Workbenches,"
echo "#   ModelRegistry, Pipelines are already Managed by default)"
echo "#"
echo "#   â†’ Click 'Create'"
echo "#   â†’ â³ Wait for status: Phase = Ready (may take 2-3 minutes)"
echo "#"
echo -e "# ${RED}   DO NOT press ENTER until the DSC shows Phase: Ready${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”„ Checking RHOAI readiness..."
verify_step "DataScienceCluster exists" "oc get datasciencecluster default-dsc 2>/dev/null"
verify_step "DataScienceCluster phase is Ready" "oc get datasciencecluster default-dsc -o jsonpath='{.status.phase}' 2>/dev/null | grep -q Ready"
verify_step "Dashboard is ready" "oc get datasciencecluster default-dsc -o jsonpath='{.status.conditions[?(@.type==\"DashboardReady\")].status}' 2>/dev/null | grep -q True"
verify_step "KServe is ready" "oc get datasciencecluster default-dsc -o jsonpath='{.status.conditions[?(@.type==\"KserveReady\")].status}' 2>/dev/null | grep -q True"
verify_step "Workbenches ready" "oc get datasciencecluster default-dsc -o jsonpath='{.status.conditions[?(@.type==\"WorkbenchesReady\")].status}' 2>/dev/null | grep -q True"
verify_step "RHOAI Dashboard gateway exists" "oc get gateway data-science-gateway -n openshift-ingress 2>/dev/null"

RHOAI_URL=https://$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')

echo ""
echo "# ğŸ“‹ What's managed vs removed:"

wait

pe "oc get datasciencecluster -o yaml | grep -A1 managementState"

echo ""
echo "# âœ… RHOAI 3.0 is ready -- all components healthy"

wait
}

section_5() {
begin_section 5 "ğŸ›¡ï¸ " "Hardware Profile with GPU Toleration" || return 0
# Depends on: RHOAI installed, RHOAI_URL set (Section 4)
verify_step "RHOAI operator is installed" "oc get csv -A 2>/dev/null | grep rhods | grep -q Succeeded"
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
echo "#"
echo "# ğŸ”‘ Problem: GPUs are tainted, so nothing can schedule there."
echo "#   But data scientists need GPU access for training and inference."
echo "#   How do you give ML workloads GPU access without opening"
echo "#   the floodgates to every pod in the cluster?"
echo "#"
echo "# ğŸ›¡ï¸  HardwareProfile = RHOAI's answer"
echo "#   â€¢ Defines resource requests (CPU, memory, GPU count)"
echo "#   â€¢ Includes the toleration to get past the GPU taint"
echo "#   â€¢ Data scientists pick a profile, not raw resource numbers"

wait

echo ""
echo "# ğŸ“‹ Here's what the HardwareProfile looks like:"

wait

pe "bat --style=grid,numbers manifests/hardware-profile.yaml"

echo ""
echo "# ğŸ”§ Two ways to create this profile:"
echo "#"
echo "#   Option A: Apply the manifest (oc apply)"
echo "#   Option B: Create it manually in the RHOAI Dashboard"
echo "#     â†’ Settings â†’ Hardware profiles â†’ 'Create hardware profile'"
echo "#     â†’ Name: nvidia-gpu"
echo "#     â†’ Add identifiers: CPU (2 default), Memory (8Gi), nvidia.com/gpu (1)"
echo "#     â†’ Add toleration: key=nvidia.com/gpu, effect=NoSchedule, operator=Exists"
echo ""
read -p "  Apply manifest now? (y/n): " HP_CHOICE
if [ "$HP_CHOICE" = "y" ]; then
  pe "oc apply -f manifests/hardware-profile.yaml"
else
  echo ""
  echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  echo -e "# ${RED}   ACTION REQUIRED -- Create HardwareProfile in RHOAI Dashboard${COLOR_RESET}"
  echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  echo "#"
  echo "# ğŸŒ RHOAI Dashboard â†’ Settings â†’ Hardware profiles"
  echo "#   â†’ Click 'Create hardware profile'"
  echo "#   â†’ Name: nvidia-gpu"
  echo "#   â†’ Add identifiers:"
  echo "#     â€¢ CPU:            default=2, min=1, max=8"
  echo "#     â€¢ Memory:         default=8Gi, min=2Gi, max=32Gi"
  echo "#     â€¢ nvidia.com/gpu: default=1, min=1, max=2 (type: Accelerator)"
  echo "#   â†’ Node scheduling â†’ Add toleration:"
  echo "#     â€¢ Key: nvidia.com/gpu"
  echo "#     â€¢ Effect: NoSchedule"
  echo "#     â€¢ Operator: Exists"
  echo "#   â†’ Click 'Create'"
  echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  wait
fi

echo ""
verify_step "HardwareProfile 'nvidia-gpu' exists" "oc get hardwareprofile nvidia-gpu -n redhat-ods-applications 2>/dev/null"
verify_manifest "HardwareProfile config" "manifests/hardware-profile.yaml"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Verify HardwareProfile in browser${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ Settings â†’ Hardware profiles"
echo "#   â†’ 'NVIDIA GPU (A10G)' should appear"
echo "#   â†’ Click it to verify:"
echo "#     â€¢ CPU: 2 (1-8)"
echo "#     â€¢ Memory: 8Gi (2Gi-32Gi)"
echo "#     â€¢ nvidia.com/gpu: 1 (1-2)"
echo "#     â€¢ Toleration: nvidia.com/gpu NoSchedule"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait
}

section_6() {
begin_section 6 "ğŸŒŸ" "Model Catalog â€” Deploy Granite LLM" || return 0
# Depends on: RHOAI installed (Section 4), HardwareProfile (Section 5)
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
verify_step "HardwareProfile exists" "oc get hardwareprofile nvidia-gpu -n redhat-ods-applications 2>/dev/null"
echo "#"
echo "# ğŸŒŸ Problem: where do you get foundation models you can trust?"
echo "#   Public model hubs have thousands of models -- no validation,"
echo "#   no support, no supply chain security. In regulated industries"
echo "#   you need models that are tested, signed, and supported."
echo "#"
echo "# ğŸ“¦ Model Catalog = Red Hat's curated model supply chain"
echo "#   â€¢ Red Hat AI Validated: tested, supported, enterprise-ready"
echo "#   â€¢ Delivered as OCI ModelCar images (same standard as containers)"
echo "#   â€¢ Immutable, version-tagged, pulled by the container runtime"
echo "#   â€¢ One-click deploy from the Dashboard -- no S3 needed"
echo "#   â€¢ You can package your own models as ModelCar images too"
echo "#"
echo "# ğŸ¯ We'll deploy Granite 3.1 8B Instruct (W4A16 quantized)"
echo "#   â€¢ IBM's enterprise LLM -- instruction-tuned for chat"
echo "#   â€¢ W4A16 = 4-bit weights, 16-bit activations"
echo "#   â€¢ Fits on our A10G (24GB VRAM)"
echo "#   â€¢ Served via vLLM -- high-performance LLM inference engine"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create granite-demo project${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ 'Data Science Projects' (left sidebar)"
echo "#   â†’ Click 'Create data science project'"
echo "#   â†’ Name: granite-demo"
echo "#   â†’ Click 'Create'"
echo "#"
echo "# ğŸ’¡ A Data Science Project = an OpenShift namespace with RHOAI labels."
echo "#   The deploy dialog can only target existing projects."
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

verify_step "granite-demo namespace exists" "oc get namespace granite-demo 2>/dev/null"

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Deploy Granite from Model Catalog${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ Model Catalog (left sidebar)"
echo "#   â†’ Browse the catalog -- show the audience what's available"
echo "#   â†’ Find: Granite 3.1 8B Instruct (quantized W4A16)"
echo "#   â†’ Click the model card â†’ review description, license"
echo "#   â†’ Click 'Deploy'"
echo "#"
echo "# ğŸ“ Deployment settings:"
echo "#   â†’ Model name:       granite-llm"
echo "#   â†’ Project:           granite-demo (created above)"
echo "#   â†’ Serving runtime:   vLLM NVIDIA GPU ServingRuntime for KServe"
echo "#   â†’ Hardware profile:  nvidia-gpu (NVIDIA GPU A10G)"
echo "#   â†’ Model location:    should be pre-filled from catalog"
echo "#     oci://registry.redhat.io/rhelai1/modelcar-granite-3-1-8b-instruct-quantized-w4a16:1.5"
echo "#   â†’ Additional serving runtime arguments:"
echo "#     --max-model-len=4096"
echo "#     (default 131K context needs more KV cache than A10G 24GB can hold)"
echo "#   â†’ Advanced settings:"
echo "#     â€¢ External route: UNCHECKED (internal only)"
echo "#     â€¢ Token auth: UNCHECKED"
echo "#   â†’ Click 'Deploy'"
echo "#"
echo "# â³ The model image will start pulling. This takes a few minutes"
echo "#   if not pre-warmed. We'll fill the time in the next section!"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”„ Verify the deployment started:"

wait

pe "oc get inferenceservice -n granite-demo"

echo ""
echo "# â³ Model is pulling/loading. Let's talk about serving runtimes"
echo "#   and backing services while we wait..."

wait
}

section_7() {
begin_section 7 "ğŸ”§" "Serving Runtimes & Backing Services" || return 0
# Depends on: RHOAI installed (Section 4)
verify_step "RHOAI operator is installed" "oc get csv -A 2>/dev/null | grep rhods | grep -q Succeeded"
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
echo "#"
echo "# ğŸ–¥ï¸  Problem: different model types need different inference engines."
echo "#   An LLM needs token-by-token generation with KV caching (vLLM)."
echo "#   A fraud detection model needs batch tensor inference (Triton)."
echo "#   You need both on the same platform."
echo "#"
echo "# ğŸ”§ Serving Runtimes = pluggable inference engines"
echo "#   â€¢ vLLM: LLM inference -- what Granite uses"
echo "#   â€¢ Triton: multi-framework ML (TensorFlow, ONNX, PyTorch)"
echo "#   â€¢ In the llm-d lab you'll see advanced LLM serving patterns"
echo "#     (disaggregated prefill/decode, routing, autoscaling)"
echo "#"
echo "# ğŸ“¦ RHOAI 3.0 stores runtimes as OpenShift Templates"
echo "#   â€¢ Dashboard discovers them in redhat-ods-applications"
echo "#   â€¢ Template wraps a bare ServingRuntime + metadata:"
echo "#     - API protocol (REST vs gRPC)"
echo "#     - Model type (predictive vs generative AI)"

wait

echo ""
echo "# ğŸ”§ vLLM is already available (built into RHOAI 3.0)"
echo "#   We used it to deploy Granite in the previous section."
echo "#"
echo "# ğŸ“‹ Now let's add Triton for custom ML models (TensorFlow, etc.)"
echo "#   We'll use this later when we deploy our own trained model."

wait

echo ""
echo "# ğŸ“‹ Here's the Triton ServingRuntime definition:"

wait

pe "bat --style=grid,numbers manifests/serving-runtime.yaml"

echo ""
echo "# ğŸ”§ Two ways to create this runtime:"
echo "#"
echo "#   Option A: Apply the Template manifest (oc apply)"
echo "#     â†’ Applies the pre-wrapped Template directly"
echo "#"
echo "#   Option B: Paste bare YAML in the RHOAI Dashboard"
echo "#     â†’ Dashboard asks for protocol + model type, wraps it for you"
echo ""
read -p "  Apply template manifest now? (y/n): " SR_CHOICE
if [ "$SR_CHOICE" = "y" ]; then
  pe "oc apply -f manifests/serving-runtime-template.yaml"
else
  echo ""
  echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  echo -e "# ${RED}   ACTION REQUIRED -- Create ServingRuntime in RHOAI Dashboard${COLOR_RESET}"
  echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  echo "#"
  echo "# ğŸ“ YAML to paste: manifests/serving-runtime.yaml"
  echo "#    (scroll up or open in another terminal)"
  echo "#"
  echo "# ğŸŒ RHOAI Dashboard â†’ Settings â†’ Serving runtimes"
  echo "#   â†’ Click 'Add serving runtime'"
  echo "#   â†’ API protocol: REST"
  echo "#     (Triton config uses HTTP only -- --allow-grpc=false)"
  echo "#   â†’ Model type: Predictive model"
  echo "#     (traditional ML: TensorFlow/Keras/ONNX, not LLM inference)"
  echo "#   â†’ Select 'Start from scratch'"
  echo "#   â†’ Paste the full YAML from manifests/serving-runtime.yaml"
  echo "#   â†’ Click 'Create'"
  echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
  wait
fi

echo ""
verify_step "ServingRuntime template exists" "oc get template triton-kserve-gpu-template -n redhat-ods-applications 2>/dev/null"

echo ""
echo "# âœ… Two serving runtimes available:"
echo "#   â€¢ vLLM â†’ LLMs (Granite, Llama, Mistral)"
echo "#   â€¢ Triton â†’ custom ML (TensorFlow, ONNX, PyTorch)"

wait

echo ""
echo "# ğŸ§± Now let's check the backing services (deployed during setup):"
echo "#"
echo "#   ğŸ“¦ MinIO â†’ S3-compatible object storage"
echo "#     â€¢ Model files, pipeline artifacts"
echo "#     â€¢ Production = AWS S3 / Ceph / ODF"
echo "#"
echo "#   ğŸ—„ï¸  MySQL â†’ Model Registry metadata"
echo "#     â€¢ Name, version, artifact paths"
echo "#     â€¢ NOT the models -- just the catalog"

wait

echo ""
echo "# ğŸ”„ Verify they're running:"

wait

pe "oc get pods -l app=minio"

pe "oc get pods -n rhoai-model-registry"

echo ""
verify_step "MinIO pod is Running" "oc get pods -l app=minio -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q Running"
verify_step "Model Registry DB pod is Running" "oc get pods -n rhoai-model-registry -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q Running"

echo ""
echo "# ğŸª£ Time to create our model storage bucket in MinIO!"
echo "#   â†’ This is where our custom trained models will land"

wait

verify_step "MinIO UI route exists" "oc get route minio-ui 2>/dev/null"

MINIO_URL=$(oc get route minio-ui -o jsonpath='https://{.spec.host}')

$BROWSER_OPEN $MINIO_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create 'models' bucket in MinIO${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ MinIO Console:"
echo "#   â†’ Login:  Username: minio  |  Password: minio123"
echo "#   â†’ Sidebar â†’ 'Object Browser'"
echo "#   â†’ Click 'Create a Bucket'"
echo "#   â†’ Bucket name: models"
echo "#   â†’ Click 'Create Bucket'"
echo "#   â†’ Leave it empty -- notebook will upload here later"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”„ Let's check on the Granite deployment while we're here:"

wait

pe "oc get inferenceservice -n granite-demo"

echo ""
echo "# ğŸ’¡ While Granite loads, a look at model storage:"
echo "#   â€¢ S3 (MinIO, AWS, Ceph) â†’ fast iteration during development"
echo "#   â€¢ OCI ModelCar images â†’ immutable, versioned, production-ready"
echo "#   â€¢ Any model can use either path -- catalog models just ship as ModelCar"

wait
}

section_8() {
begin_section 8 "ğŸ’¬" "LlamaStack + Chat with Granite" || return 0
# Depends on: Granite deployed (Section 6)
echo "#"
echo "# ğŸ’¬ Let's prove Granite is actually working -- chat with it!"
echo "#   â€¢ LlamaStack gives us a chat UI on top of the model"
echo "#   â€¢ You'll go deep on LlamaStack in the agentic and RAG labs"
echo "#   â€¢ For now, it's just here to show live inference"

wait

echo ""
echo "# ğŸ”„ First, let's make sure Granite is ready:"

wait

pe "oc get inferenceservice -n granite-demo"

verify_step "Granite InferenceService is Ready" "oc get inferenceservice -n granite-demo -o jsonpath='{.items[0].status.conditions[?(@.type==\"Ready\")].status}' 2>/dev/null | grep -q True"

echo ""
echo "# ğŸ¯ Granite is serving! You can see it in the RHOAI Dashboard:"
echo "#   â†’ RHOAI Dashboard â†’ Data Science Projects â†’ granite-demo"
echo "#   â†’ 'Model' tab shows the deployed model, status, and endpoint"

wait

# Get the Granite internal endpoint
GRANITE_ISVC=$(oc get inferenceservice -n granite-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null) || true
GRANITE_ENDPOINT="http://${GRANITE_ISVC}-predictor.granite-demo.svc.cluster.local:8080/v1"

# Get the model ID that vLLM is serving (needed for LlamaStack config)
GRANITE_MODEL_ID=$(oc exec -n granite-demo deploy/${GRANITE_ISVC}-predictor -c kserve-container -- curl -s http://localhost:8080/v1/models 2>/dev/null | python3 -c "import sys,json; print(json.load(sys.stdin)['data'][0]['id'])" 2>/dev/null) || GRANITE_MODEL_ID="granite"

echo ""
echo "# â³ Deploying LlamaStack API server + Playground..."
echo "#   (connecting to Granite at ${GRANITE_ENDPOINT})"

# Create ConfigMap with LlamaStack run.yaml config pointing to Granite
oc create configmap llama-stack-config -n granite-demo \
  --from-literal=run.yaml="$(cat <<LLCFG
version: '2'
image_name: vllm
metadata_store:
  type: sqlite
  db_path: /tmp/llama_stack_metadata.db
apis:
  - inference
models:
  - metadata: {}
    model_id: ${GRANITE_MODEL_ID}
    provider_id: vllm
    provider_model_id: ${GRANITE_MODEL_ID}
    model_type: llm
providers:
  inference:
    - provider_id: vllm
      provider_type: "remote::vllm"
      config:
        url: "${GRANITE_ENDPOINT}"
        tls_verify: false
server:
  port: 8321
LLCFG
)" --dry-run=client -o yaml 2>/dev/null | oc apply -f - 2>/dev/null

# Deploy LlamaStack server + playground
oc apply -f "${DEMO_DIR}/manifests/llama-stack.yaml" -n granite-demo 2>/dev/null
oc apply -f "${DEMO_DIR}/manifests/llama-stack-playground.yaml" -n granite-demo 2>/dev/null

# Set the correct model ID on the playground
oc set env deployment/llama-stack-playground -n granite-demo \
  DEFAULT_MODEL="${GRANITE_MODEL_ID}" 2>/dev/null

verify_step "LlamaStack server is Running" "oc get pods -n granite-demo -l app=llama-stack -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q Running"
verify_step "Playground is Running" "oc get pods -n granite-demo -l app=llama-stack-playground -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q Running"

echo ""
echo "# ğŸŒ Opening the Playground..."

wait

PLAYGROUND_URL=$(oc get route llama-stack-playground -n granite-demo -o jsonpath='https://{.spec.host}')

$BROWSER_OPEN $PLAYGROUND_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Chat with Granite!${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ In the Playground:"
echo "#   â†’ Select model: ${GRANITE_MODEL_ID}"
echo "#   â†’ Try these FSI-relevant prompts:"
echo "#"
echo "#   ğŸ’¬ 'Explain the key components of Basel III capital requirements'"
echo "#   ğŸ’¬ 'What are the main risks in algorithmic trading?'"
echo "#   ğŸ’¬ 'Summarize PCI-DSS compliance requirements for payment processing'"
echo "#   ğŸ’¬ 'What is model risk management and why does it matter in banking?'"
echo "#"
echo "# ğŸ”‘ Key points for the audience:"
echo "#   â€¢ This model is running on our A10G GPU, on OpenShift"
echo "#   â€¢ Enterprise-grade: Red Hat validated, IBM-developed"
echo "#   â€¢ No data leaves the cluster -- internal inference only"
echo "#   â€¢ From catalog browse to live chat in minutes"
echo "#   â€¢ In the labs you'll build RAG and agents on top"
echo "#     of this same infrastructure"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# âœ… Foundation model deployed from catalog and serving live!"
echo "#   Next: build and deploy your OWN custom model"

wait
}

section_9() {
begin_section 9 "ğŸ§ª" "Workbench & Train Custom Model" || return 0
# Depends on: RHOAI_URL (Section 4), MINIO_URL (Section 7),
#             HardwareProfile (Section 5), ServingRuntime (Section 7)
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
ensure_var MINIO_URL "oc get route minio-ui -o jsonpath='https://{.spec.host}'"
verify_step "HardwareProfile exists" "oc get hardwareprofile nvidia-gpu -n redhat-ods-applications 2>/dev/null"
verify_step "ServingRuntime template exists" "oc get template triton-kserve-gpu-template -n redhat-ods-applications 2>/dev/null"
echo "#"
echo "# ğŸ”€ Granite gave us GenAI out of the box -- but foundation"
echo "#   models can't do everything. FSI needs custom models for"
echo "#   fraud detection, credit scoring, risk pricing -- problems"
echo "#   that require your proprietary data and domain expertise."
echo "#"
echo "# ğŸ§ª Problem: data scientists need GPU environments, but you"
echo "#   don't want them SSH'ing into bare metal or fighting over"
echo "#   shared Jupyter servers. RHOAI workbenches give each team"
echo "#   an isolated, self-service GPU environment with S3 access."
echo "#"
echo "# ğŸ¯ Interactive ML workflow:"
echo "#   1ï¸âƒ£  Create Data Science Project"
echo "#   2ï¸âƒ£  Connect S3 storage"
echo "#   3ï¸âƒ£  Launch GPU workbench"
echo "#   4ï¸âƒ£  Train model + upload to MinIO"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create Data Science Project${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# 1ï¸âƒ£  ğŸŒ RHOAI Dashboard:"
echo "#   â†’ 'Data Science Projects' in left sidebar"
echo "#   â†’ Click 'Create data science project'"
echo "#   â†’ Name: fsi-demo"
echo "#   â†’ Click 'Create'"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# âš™ï¸  While we're here, let's deploy the pipeline server"
echo "#   so it's ready when we get to Section 12."
echo "#   This takes a couple minutes to start up."
echo "#"
echo "# ğŸ“¦ The pipeline server reuses our MinIO S3 storage"
echo "#   â€¢ Same 'models' bucket we created earlier"
echo "#   â€¢ Artifacts stored under the pipeline-artifacts/ prefix"
echo "#   â€¢ No extra bucket needed -- pipelines and models share storage"
echo "#   â€¢ You can also configure this from the RHOAI Dashboard"
echo "#     (Data Science Projects â†’ Pipelines â†’ Configure pipeline server)"

wait

pe "oc apply -f manifests/dspa.yaml"

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create S3 Connection${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# 2ï¸âƒ£  Inside fsi-demo project â†’ 'Connections' tab"
echo "#   â†’ Click 'Create connection'"
echo "#   â†’ Connection type: S3 compatible object storage - v1"
echo "#   â†’ Connection name:  minio-models"
echo "#   â†’ Access key:       minio"
echo "#   â†’ Secret key:       minio123"
echo "#   â†’ Endpoint:         http://minio-service.default.svc.cluster.local:9000"
echo "#   â†’ Bucket:           models"
echo "#   â†’ Click 'Create'"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Create GPU Workbench${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# 3ï¸âƒ£  'Workbenches' tab â†’ 'Create workbench'"
echo "#   â†’ Name: gpu-workbench"
echo "#   â†’ Image: TensorFlow (select CUDA variant if available)"
echo "#   â†’ Hardware profile: nvidia-gpu"
echo "#   â†’ Connections â†’ check 'Attach existing connections'"
echo "#     â†’ Select: minio-models"
echo "#   â†’ Click 'Create workbench'"
echo "#   â†’ â³ Wait for status: Running"
echo "#   â†’ Click 'Open' to launch JupyterLab"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Run notebooks in JupyterLab${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# 4ï¸âƒ£  In JupyterLab terminal, clone the repo:"
echo "#   â†’ git clone https://github.com/adam-d-young/RHOAI-demo.git"
echo "#   â†’ Navigate to RHOAI-demo/notebooks/"
echo "#   â†’ Run in order:"
echo "#"
echo "#   ğŸ““ gpu-check.py        â†’ Can TensorFlow see the A10G?"
echo "#   ğŸ““ gpu-demo.py         â†’ GPU matrix multiply"
echo "#   ğŸ““ train-and-upload.py â†’ Train model, upload to MinIO"
echo "#"
echo -e "# ${RED}   DO NOT continue until train-and-upload.py completes${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

$BROWSER_OPEN $MINIO_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Verify model in MinIO${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# 5ï¸âƒ£  ğŸŒ MinIO Console:"
echo "#   â†’ Object Browser â†’ 'models' bucket"
echo "#   â†’ You should see: production/demo-model/"
echo "#     â†’ config.pbtxt                          -- Triton serving config (inputs, outputs, platform)"
echo "#     â†’ 1/model.savedmodel/saved_model.pb     -- TensorFlow graph (model architecture + ops)"
echo "#     â†’ 1/model.savedmodel/fingerprint.pb     -- Model hash for version tracking"
echo "#     â†’ 1/model.savedmodel/variables/          -- Trained weights and biases"
echo "#"
echo "# âœ… Model trained on GPU, exported, and stored in S3"
echo "#   Next: register it in the Model Registry before deploying"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait
}

section_10() {
begin_section 10 "ğŸ“‹" "Model Registry" || return 0
# Depends on: RHOAI installed (Section 4), Model trained (Section 9),
#             MySQL DB deployed (setup.sh Step 8)
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
verify_step "RHOAI operator is installed" "oc get csv -A 2>/dev/null | grep rhods | grep -q Succeeded"
verify_step "Model Registry DB is running" "oc get pods -n rhoai-model-registry -o jsonpath='{.items[0].status.phase}' 2>/dev/null | grep -q Running"
echo "#"
echo "# ğŸ—‚ï¸  Problem: a model works in my notebook -- now what?"
echo "#   Who trained it? On what data? Is this the version in production?"
echo "#   In FSI, regulators ask these questions. Without answers,"
echo "#   you fail audits (SR 11-7, SS1/23, PCI-DSS)."
echo "#"
echo "# ğŸ“‹ Model Registry = governance + lineage for YOUR models"
echo "#   â€¢ NOT the Model Catalog (that's pre-built Red Hat models)"
echo "#   â€¢ Tracks: name, version, artifact location, custom properties"
echo "#   â€¢ Deploy directly from the registry -- full audit trail"
echo "#   â€¢ Every deployment is linked back to a registered version"

wait

echo ""
echo "# ğŸ”§ First, create the Model Registry instance"
echo "#   â€¢ The RHOAI operator installs the registry capability"
echo "#   â€¢ But we still need to create an actual registry instance"
echo "#   â€¢ It connects to our MySQL backend (deployed in setup)"

wait

echo ""
echo "# ğŸ“‹ Here's the registry instance manifest:"

wait

pe "bat --style=grid,numbers manifests/model-registry-instance.yaml"

wait

pe "oc apply -f manifests/model-registry-instance.yaml"

echo ""
echo "# â³ Waiting for registry to become available..."

wait

pe "oc wait --for=condition=Available mr/fsi-model-registry -n rhoai-model-registries --timeout=120s"

verify_step "Model Registry instance is Available" "oc get mr fsi-model-registry -n rhoai-model-registries -o jsonpath='{.status.conditions[?(@.type==\"Available\")].status}' 2>/dev/null | grep -q True"

echo ""
echo "# âœ… Registry is live! Now register our trained model"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Register model in Model Registry${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ 'Model Registry' in left sidebar"
echo "#   â†’ Select registry: fsi-model-registry"
echo "#   â†’ Click 'Register model'"
echo "#"
echo "# ğŸ“ Model details:"
echo "#   â†’ Model name:        fsi-fraud-detection"
echo "#   â†’ Model description:"
echo "#     Binary classifier for real-time transaction fraud detection."
echo "#     5-feature input (amount, category, time delta, account age,"
echo "#     frequency). Sigmoid output (0-1), >0.5 = suspected fraud."
echo "#"
echo "# ğŸ“¦ Version details:"
echo "#   â†’ Version name:      v1.0"
echo "#   â†’ Version description:"
echo "#     Initial release. Trained on 100K synthetic transactions."
echo "#     Architecture: 5â†’10(ReLU)â†’1(Sigmoid). Validation AUC: 0.94."
echo "#"
echo "# ğŸ”— Model location:"
echo "#   â†’ Source model format:  tensorflow"
echo "#   â†’ Source model version: 2"
echo "#   â†’ Select 'Object storage' (not URI)"
echo "#     Endpoint: http://minio-service.default.svc.cluster.local:9000"
echo "#     Bucket:   models"
echo "#     Region:   us-east-1"
echo "#     Path:     production/"
echo "#"
echo "#   â†’ Click 'Register model'"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Add custom properties${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ Click into 'fsi-fraud-detection' â†’ 'v1.0' version"
echo "#   â†’ Look for 'Properties' or 'Custom properties' section"
echo "#   â†’ Add these key-value pairs:"
echo "#"
echo "#   Key                    Value"
echo "#   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo "#   team                   FSI Risk Analytics"
echo "#   use_case               Real-time fraud detection"
echo "#   regulatory_framework   PCI-DSS, SOX"
echo "#   data_classification    Confidential - PII Adjacent"
echo "#   owner                  Adam Young"
echo "#   gpu_type               NVIDIA A10G (24GB VRAM)"
echo "#   serving_runtime        NVIDIA Triton 24.01"
echo "#   training_dataset       synthetic_transactions_100k"
echo "#   validation_auc         0.94"
echo "#   risk_tier              Tier 2 - Model risk review complete"
echo "#   approval_status        Approved for staging"
echo "#"
echo "# ğŸ’¡ Why this matters in FSI:"
echo "#   â€¢ Regulators can audit which model version is in production"
echo "#   â€¢ Risk teams see validation metrics + approval status"
echo "#   â€¢ Data governance tracks PII-adjacent classifications"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# âœ… Model registered with full metadata"
echo "#   Next: deploy it directly from the registry"

wait
}

section_11() {
begin_section 11 "ğŸš€" "Deploy from Registry & Test Inference" || return 0
# Depends on: Model registered (Section 10), ServingRuntime (Section 7)
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
verify_step "ServingRuntime template exists" "oc get template triton-kserve-gpu-template -n redhat-ods-applications 2>/dev/null"
echo "#"
echo "# ğŸš€ Problem: how do you go from 'model works in my notebook'"
echo "#   to 'model serves production traffic on a GPU'?"
echo "#   Manually wiring up storage paths, serving runtimes, and"
echo "#   hardware profiles is error-prone and unauditable."
echo "#"
echo "# ğŸ“¦ Deploy from Registry = one-click production deployment"
echo "#   â€¢ Registry pre-fills artifact URI, format, and version"
echo "#   â€¢ Deployment is tracked in the registry's Deployments tab"
echo "#   â€¢ Full lineage: trained â†’ registered â†’ deployed â†’ serving"

wait

echo ""
echo "# ğŸ”§ First, we need to free a GPU for our fraud model."
echo "#   Both A10G GPUs are currently in use:"
echo "#     GPU 1: Granite LLM (granite-demo) -- done after Section 8"
echo "#     GPU 2: Training workbench (fsi-demo) -- still needed"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Stop the Granite deployment${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ Data Science Projects â†’ granite-demo"
echo "#   â†’ Models tab â†’ click the Granite model kebab menu (â‹®)"
echo "#   â†’ 'Delete model server'"
echo "#   â†’ Confirm deletion"
echo "#"
echo "# ğŸ’¡ We're done chatting -- time to deploy our own model."
echo "#   Deleting the model server frees the GPU."
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

GRANITE_ISVC=$(oc get inferenceservice -n granite-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null) || true
if [ -n "$GRANITE_ISVC" ]; then
  echo ""
  echo "# â³ Waiting for Granite to scale down..."
  while oc get pods -n granite-demo -l serving.kserve.io/inferenceservice=${GRANITE_ISVC} --no-headers 2>/dev/null | grep -q Running; do
    sleep 5
  done
fi
echo -e "  ${GREEN}âœ… GPU freed${COLOR_RESET}"

# Also scale down LlamaStack (no longer needed without the LLM)
oc scale deployment llama-stack llama-stack-playground -n granite-demo --replicas=0 2>/dev/null || true

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Deploy model from registry${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ Model Registry â†’ fsi-model-registry"
echo "#   â†’ Click 'fsi-fraud-detection'"
echo "#   â†’ On version 'v1.0' row â†’ click 'Deploy' (kebab menu or button)"
echo "#"
echo "# ğŸ“ Deployment settings:"
echo "#   â†’ Model name:       fsi-demo-model"
echo "#   â†’ Project:           fsi-demo"
echo "#   â†’ Serving runtime:   Triton Inference Server (GPU)"
echo "#   â†’ Model framework:   tensorflow - 2"
echo "#   â†’ Model location:    should be pre-filled from registry"
echo "#     (if not: Existing connection â†’ minio-models, path: production)"
echo "#   â†’ Advanced settings:"
echo "#     â€¢ External route: UNCHECKED"
echo "#     â€¢ Token auth: UNCHECKED"
echo "#   â†’ Click 'Deploy'"
echo "#   â†’ â³ Wait for status: âœ… green checkmark"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”„ Verify deployment from CLI while we wait:"

wait

pe "oc get inferenceservice -n fsi-demo"

# Capture the InferenceService name (Dashboard may auto-generate it from registry)
ISVC_NAME=$(oc get inferenceservice -n fsi-demo -o jsonpath='{.items[0].metadata.name}' 2>/dev/null) || true
echo ""
echo "# â³ Waiting for model to load on GPU..."
echo "#   InferenceService name: ${ISVC_NAME:-unknown}"

verify_step "InferenceService is Ready" "oc get inferenceservice -n fsi-demo -o jsonpath='{.items[0].status.conditions[?(@.type==\"Ready\")].status}' 2>/dev/null | grep -q True"

echo ""
echo "# ğŸ” Check the registry -- Deployments tab should now show this deployment"
echo "#   â†’ Go back to Model Registry â†’ fsi-fraud-detection"
echo "#   â†’ Click 'Deployments' tab"
echo "#   â†’ ${ISVC_NAME:-the deployment} should appear with status"

wait

echo ""
echo "# âœ… Model deployed from registry with full lineage tracking"
echo "#   Now let's send some predictions!"

wait

echo ""
echo "# ğŸ¯ The payoff -- send data to the live model and get a prediction!"
echo "#   â€¢ Our model: 5 floats in â†’ 1 sigmoid probability out"
echo "#   â€¢ Using Triton's v2 REST API from inside the cluster"
echo "#   â€¢ The notebook auto-detects the input tensor name"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Run inference notebook${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ In JupyterLab (same workbench from Section 9):"
echo "#   â†’ Navigate to RHOAI-demo/notebooks/"
echo "#   â†’ Open: ğŸ““ inference-test.ipynb"
echo "#"
echo -e "# ${CYAN}âœï¸  FIRST: Update ISVC_NAME in the first code cell:${COLOR_RESET}"
echo "#     ISVC_NAME = \"${ISVC_NAME:-<check oc get inferenceservice>}\""
echo "#"
echo "#   â†’ Then run each cell with Shift+Enter"
echo "#"
echo "# ğŸ’¡ What it does:"
echo "#   1. Queries Triton for model metadata (auto-detects tensor names)"
echo "#   2. Sends two different prediction requests"
echo "#   3. Shows the sigmoid probability output (0-1)"
echo "#"
echo "# ğŸ”‘ In production this would be:"
echo "#   â€¢ Fraud detection scores on transactions"
echo "#   â€¢ Credit risk assessments"
echo "#   â€¢ Real-time pricing models"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# âœ… Full custom model lifecycle complete:"
echo "#   Train on GPU â†’ register â†’ deploy from registry â†’ live inference"

wait
}

section_12() {
begin_section 12 "âš™ï¸ " "Data Science Pipelines & Experiments" || return 0
# Depends on: RHOAI installed (Section 4), fsi-demo namespace (Section 9)
ensure_var RHOAI_URL "echo https://\$(oc get gateway data-science-gateway -n openshift-ingress -o jsonpath='{.spec.listeners[0].hostname}')"
verify_step "fsi-demo namespace exists" "oc get namespace fsi-demo 2>/dev/null"
echo "#"
echo "# âš™ï¸  Problem: we just trained and deployed a model manually."
echo "#   That works once. But what about next week when new data arrives?"
echo "#   Or when a different team needs the same workflow?"
echo "#   Manual steps don't scale, and they don't leave an audit trail."
echo "#"
echo "# ğŸ”§ Data Science Pipelines = automated, repeatable ML workflows"
echo "#   â€¢ Kubeflow Pipelines (KFP) on OpenShift"
echo "#   â€¢ Each step = a container with defined inputs/outputs"
echo "#   â€¢ Triggered on schedule, git push, or new data"
echo "#   â€¢ Every run is tracked: parameters, logs, artifacts, duration"
echo "#"
echo "# ğŸ“‹ Our pipeline automates what we did manually:"
echo "#   1ï¸âƒ£  Data Processing   â†’ generate/clean transaction data"
echo "#   2ï¸âƒ£  Feature Extract   â†’ normalize features"
echo "#   3ï¸âƒ£  Train Model       â†’ fit classifier"
echo "#   4ï¸âƒ£  Upload Model      â†’ push artifacts to S3"

wait

echo ""
echo "# ğŸ”§ Pipeline server was deployed back in Section 9."
echo "#   Let's verify it's ready:"

wait

verify_step "DSPA is Ready" "oc get dspa dspa -n fsi-demo -o jsonpath='{.status.conditions[?(@.type==\"Ready\")].status}' 2>/dev/null | grep -q True"

pe "oc get dspa -n fsi-demo"

wait

echo ""
echo "# ğŸ”§ Step 1: Compile the pipeline"
echo "#   â€¢ Pipeline is written in Python using KFP v2 SDK"
echo "#   â€¢ Each @dsl.component becomes a container step"
echo "#   â€¢ Compiling produces an IR YAML (Intermediate Representation)"
echo "#"
echo "# ğŸ“‹ What is IR YAML?"
echo "#   â€¢ Platform-agnostic pipeline specification"
echo "#   â€¢ Python SDK â†’ compiles â†’ IR YAML â†’ imported into RHOAI"
echo "#   â€¢ The DSPA backend translates IR YAML into an Argo Workflow"
echo "#   â€¢ Same IR works on any KFP v2-compatible backend"
echo "#"
echo "# ğŸ“‹ Our pipeline has 4 steps:"
echo "#   data-processing â†’ feature-extract â†’ train-model â†’ upload-model"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Compile pipeline in workbench${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ In JupyterLab (same workbench from Section 9):"
echo "#   â†’ Navigate to RHOAI-demo/notebooks/"
echo "#   â†’ Open: ğŸ““ fsi-fraud-pipeline.ipynb"
echo "#   â†’ Walk through the cells -- each one defines a pipeline step"
echo "#   â†’ Run all cells (Shift+Enter through each)"
echo "#   â†’ The last cell compiles and outputs: fsi-fraud-pipeline.yaml"
echo "#   â†’ You should see: 'Pipeline compiled to: fsi-fraud-pipeline.yaml'"
echo "#"
echo "#   ğŸ’¡ The Python code defines the pipeline declaratively."
echo "#     The compiler serializes it to IR YAML -- the portable format"
echo "#     that any KFP v2 backend can execute."
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ”§ Step 2: Import and run the 4-step pipeline"

wait

$BROWSER_OPEN $RHOAI_URL

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Import pipeline in RHOAI Dashboard${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ fsi-demo project â†’ 'Pipelines' tab"
echo "#   â†’ Click 'Import pipeline'"
echo "#   â†’ Pipeline name: FSI Fraud Detection Training"
echo "#   â†’ Upload: fsi-fraud-pipeline.yaml (the IR YAML from workbench)"
echo "#     (download from JupyterLab or copy/paste)"
echo "#   â†’ Click 'Import pipeline'"
echo "#"
echo "# ğŸƒ Then create a run:"
echo "#   â†’ Click the pipeline name â†’ 'Create run'"
echo "#   â†’ Run name: fraud-training-run-1"
echo "#   â†’ Experiment: Create new â†’ 'fsi-fraud-experiments'"
echo "#   â†’ Parameters: num_samples = 10000 (default)"
echo "#   â†’ Click 'Create'"
echo "#"
echo "# ğŸ‘€ Watch the pipeline execute:"
echo "#   â†’ Each step lights up as it runs"
echo "#   â†’ Click a step to see its logs"
echo "#   â†’ 4 steps run in sequence:"
echo "#     data-processing â†’ feature-extract â†’ train-model â†’ upload-model"
echo "#"
echo "# ğŸ’¡ In production:"
echo "#   â€¢ Submit pipelines programmatically (kfp.client.Client)"
echo "#   â€¢ Trigger from CI/CD (Tekton, GitHub Actions) on git push"
echo "#   â€¢ Schedule recurring runs (daily retraining on new data)"
echo "#   â€¢ Add validation, monitoring, and promotion steps"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# ğŸ¨ Note: Elyra (visual pipeline editor)"
echo "#   â€¢ RHOAI workbenches also include Elyra"
echo "#   â€¢ Drag-and-drop pipeline building -- no Python SDK needed"
echo "#   â€¢ Each node = a notebook or Python script"
echo "#   â€¢ File â†’ New â†’ Pipeline Editor to try it"
echo "#   â€¢ Elyra produces its own .pipeline format (separate from KFP IR YAML)"
echo "#   â€¢ You'll explore Elyra in the hands-on labs"

wait

echo ""
echo "# ğŸ“Š Step 3: Experiments & Metrics"
echo "#   â€¢ Experiments group related runs for comparison"
echo "#   â€¢ Our train step logs metrics: accuracy, AUC, dataset size"
echo "#   â€¢ Click a completed run â†’ metrics appear in the visualization tab"
echo "#   â€¢ Compare AUC across runs to pick the best model"

wait

echo ""
echo -e "# ${RED}ğŸ›‘ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo -e "# ${RED}   ACTION REQUIRED -- Explore Experiments${COLOR_RESET}"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"
echo "#"
echo "# ğŸŒ RHOAI Dashboard â†’ fsi-demo project â†’ 'Experiments' tab"
echo "#   â†’ Click 'fsi-fraud-experiments'"
echo "#   â†’ Shows all runs in this experiment"
echo "#   â†’ Click a completed run to see:"
echo "#     â€¢ DAG visualization (pipeline graph)"
echo "#     â€¢ Per-step logs"
echo "#     â€¢ Metrics tab: train_accuracy, test_accuracy, auc, num_samples"
echo "#     â€¢ Input/output artifacts"
echo "#     â€¢ Run parameters and duration"
echo "#"
echo "# ğŸ’¡ In production:"
echo "#   â€¢ Run the pipeline on new data â†’ automatic retraining"
echo "#   â€¢ Compare AUC scores across experiments"
echo "#   â€¢ Promote best model to Model Registry â†’ Deploy"
echo "#   â€¢ Schedule pipelines to run on a cadence"
echo -e "# ${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${COLOR_RESET}"

wait

echo ""
echo "# âœ… Pipeline deployed, run complete, experiment tracked"
echo "#   Manual workflow (Sections 9-11) is now automated"

wait
}

section_13() {
echo ""
echo -e "# ğŸ‰ ${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}"
echo "#"
echo -e "#   ${GREEN}That's the platform. Here's what you built today:${COLOR_RESET}"
echo "#"
echo "#   ğŸ”§ GPU infrastructure â†’ managed by operators, protected by taints"
echo "#   ğŸŒŸ Foundation model â†’ deployed from catalog, chatting in minutes"
echo "#   ğŸ§ª Custom model â†’ trained on GPU, registered, deployed, serving"
echo "#   âš™ï¸  Pipeline â†’ automated the whole training workflow"
echo "#"
echo -e "#   ${CYAN}Coming up in the labs:${COLOR_RESET}"
echo "#"
echo "#   ğŸ’¬ LlamaStack deep dive â†’ agents, tool use, agentic workflows"
echo "#   ğŸ“š RAG â†’ ground LLMs in your own documents"
echo "#   ğŸ¢ MaaS â†’ models as a shared service across teams"
echo "#   ğŸš€ llm-d â†’ disaggregated LLM serving at scale"
echo "#   ğŸ›¡ï¸  TrustyAI â†’ model explainability and safety"
echo "#"
echo "#   Everything runs on the same platform you just saw."
echo "#"
echo -e "# ${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${COLOR_RESET}"
}

######################################################################
# Run all sections
######################################################################

section_1
section_2
section_3
section_4
section_5
section_6
section_7
section_8
section_9
section_10
section_11
section_12
section_13
