# ServingRuntime - Makes Triton Inference Server available in RHOAI Dashboard
# This defines a model server that can serve TensorFlow, Keras, and ONNX models.
# GPU is NOT hardcoded here -- you select it via the hardware profile
# when creating the inference service in the RHOAI UI.
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-kserve-gpu
  annotations:
    opendatahub.io/template-display-name: "Triton Inference Server (GPU)"
    opendatahub.io/template-name: "triton-kserve-gpu"
    opendatahub.io/accelerator-selector: "nvidia.com/gpu"
  labels:
    opendatahub.io/dashboard: "true"  # Makes it visible in the RHOAI dropdown
spec:
  multiModel: false                   # Single model per server instance
  supportedModelFormats:
    - name: keras
      version: "3"
    - name: tensorflow
      version: "2"
    - name: onnx
      version: "1"
  containers:
    - name: kserve-container
      image: nvcr.io/nvidia/tritonserver:24.01-py3
      args:
        - "tritonserver"
        - "--model-store=/mnt/models"   # KServe mounts the model here
        - "--http-port=8080"
        - "--allow-grpc=false"
        - "--allow-http=true"
        - "--model-control-mode=explicit"
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      resources:
        requests:
          cpu: "1"
          memory: "4Gi"
        limits:
          cpu: "2"
          memory: "8Gi"
