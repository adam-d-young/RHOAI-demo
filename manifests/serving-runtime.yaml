# Bare ServingRuntime YAML -- paste this in the RHOAI Dashboard GUI
# when adding a custom serving runtime via Settings â†’ Serving runtimes.
# The Dashboard wraps it in a Template and adds the metadata you select
# (API protocol, model type) as annotations.
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-kserve-gpu
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/runtime-version: "24.01"
    openshift.io/display-name: "Triton Inference Server (GPU)"
  labels:
    opendatahub.io/dashboard: "true"
spec:
  multiModel: false
  supportedModelFormats:
    - name: keras
      version: "3"
    - name: tensorflow
      version: "2"
    - name: onnx
      version: "1"
  containers:
    - name: kserve-container
      image: nvcr.io/nvidia/tritonserver:24.01-py3
      args:
        - "tritonserver"
        - "--model-store=/mnt/models"
        - "--http-port=8080"
        - "--allow-grpc=false"
        - "--allow-http=true"
        - "--model-control-mode=explicit"
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      resources:
        requests:
          cpu: "1"
          memory: "4Gi"
        limits:
          cpu: "2"
          memory: "8Gi"
