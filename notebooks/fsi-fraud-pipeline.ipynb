{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSI Fraud Detection Training Pipeline (KFP v2)\n",
    "\n",
    "4-step pipeline (intentionally missing the Validate step):\n",
    "1. **Data Processing** - Generate synthetic transaction data\n",
    "2. **Feature Extract** - Normalize and derive model features\n",
    "3. **Train Model** - Fit a GradientBoosting classifier\n",
    "4. **Upload Model** - Push artifacts to MinIO S3\n",
    "\n",
    "The Validate step is provided separately (`validate-model.ipynb`) and\n",
    "added visually using the Elyra pipeline editor during the demo.\n",
    "\n",
    "Run all cells, then import the generated YAML:\n",
    "**RHOAI Dashboard → Pipelines → Import pipeline → `fsi-fraud-pipeline.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl, compiler\n",
    "\n",
    "# Internal OpenShift Python image -- available on all OCP clusters\n",
    "BASE_IMAGE = \"image-registry.openshift-image-registry.svc:5000/openshift/python:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Processing\n",
    "Generate synthetic transaction data with fraud labels. Each `@dsl.component` becomes a container step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"numpy==1.26.4\", \"pandas==2.2.2\"],\n",
    ")\n",
    "def data_processing(num_samples: int, dataset: dsl.Output[dsl.Dataset]):\n",
    "    \"\"\"Generate synthetic transaction data with fraud labels.\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    np.random.seed(42)\n",
    "    n = num_samples\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        \"amount\": np.random.exponential(500, n),\n",
    "        \"category\": np.random.randint(0, 10, n),\n",
    "        \"time_delta\": np.random.exponential(3600, n),\n",
    "        \"account_age_days\": np.random.randint(1, 3650, n),\n",
    "        \"tx_frequency_7d\": np.random.poisson(5, n),\n",
    "    })\n",
    "\n",
    "    # Fraud label: high amount + new account + high frequency -> more likely\n",
    "    fraud_score = (\n",
    "        (data[\"amount\"] > 1000).astype(float) * 0.3\n",
    "        + (data[\"account_age_days\"] < 90).astype(float) * 0.3\n",
    "        + (data[\"tx_frequency_7d\"] > 10).astype(float) * 0.2\n",
    "        + np.random.uniform(0, 0.2, n)\n",
    "    )\n",
    "    data[\"is_fraud\"] = (fraud_score > 0.5).astype(int)\n",
    "\n",
    "    os.makedirs(dataset.path, exist_ok=True)\n",
    "    data.to_csv(f\"{dataset.path}/transactions.csv\", index=False)\n",
    "\n",
    "    fraud_count = data[\"is_fraud\"].sum()\n",
    "    print(f\"Generated {n} transactions ({fraud_count} fraud, {n - fraud_count} legit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction\n",
    "Normalize features to 0-1 range. Each step receives typed inputs/outputs -- KFP handles data passing between containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"numpy==1.26.4\", \"pandas==2.2.2\"],\n",
    ")\n",
    "def feature_extract(dataset: dsl.Input[dsl.Dataset], features: dsl.Output[dsl.Dataset]):\n",
    "    \"\"\"Normalize features to 0-1 range for model training.\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    data = pd.read_csv(f\"{dataset.path}/transactions.csv\")\n",
    "\n",
    "    feature_cols = [\"amount\", \"category\", \"time_delta\", \"account_age_days\", \"tx_frequency_7d\"]\n",
    "    for col in feature_cols:\n",
    "        min_val, max_val = data[col].min(), data[col].max()\n",
    "        data[f\"{col}_norm\"] = (data[col] - min_val) / (max_val - min_val + 1e-8)\n",
    "\n",
    "    norm_cols = [f\"{c}_norm\" for c in feature_cols]\n",
    "    result = data[norm_cols + [\"is_fraud\"]]\n",
    "\n",
    "    os.makedirs(features.path, exist_ok=True)\n",
    "    result.to_csv(f\"{features.path}/features.csv\", index=False)\n",
    "\n",
    "    print(f\"Extracted {len(norm_cols)} normalized features from {len(result)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model\n",
    "Train a GradientBoosting fraud classifier. Note that `packages_to_install` tells KFP what to pip install in the container at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"numpy==1.26.4\", \"pandas==2.2.2\", \"scikit-learn==1.5.0\"],\n",
    ")\n",
    "def train_model(features: dsl.Input[dsl.Dataset], model: dsl.Output[dsl.Model]):\n",
    "    \"\"\"Train a GradientBoosting fraud classifier.\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data = pd.read_csv(f\"{features.path}/features.csv\")\n",
    "    feature_cols = [c for c in data.columns if c != \"is_fraud\"]\n",
    "\n",
    "    X = data[feature_cols].values\n",
    "    y = data[\"is_fraud\"].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    with open(f\"{model.path}/model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(clf, f)\n",
    "    np.savez(f\"{model.path}/test_data.npz\", X_test=X_test, y_test=y_test)\n",
    "\n",
    "    print(f\"Train accuracy: {clf.score(X_train, y_train):.4f}\")\n",
    "    print(f\"Test accuracy:  {clf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload Model\n",
    "Push model artifacts to MinIO S3. In production this would use injected credentials, not hardcoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"boto3==1.34.0\"],\n",
    ")\n",
    "def upload_model(model: dsl.Input[dsl.Model]):\n",
    "    \"\"\"Upload model artifacts to MinIO S3.\"\"\"\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.default.svc.cluster.local:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    bucket = \"models\"\n",
    "    prefix = \"pipeline-output\"\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(model.path):\n",
    "        for filename in filenames:\n",
    "            local_path = os.path.join(dirpath, filename)\n",
    "            relative = os.path.relpath(local_path, model.path)\n",
    "            s3_key = f\"{prefix}/{relative}\"\n",
    "            print(f\"Uploading: s3://{bucket}/{s3_key}\")\n",
    "            s3.upload_file(local_path, bucket, s3_key)\n",
    "\n",
    "    print(f\"\\nModel uploaded to s3://{bucket}/{prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline & Compile\n",
    "Wire the steps together and compile to IR YAML. The output file is what gets imported into RHOAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"FSI Fraud Detection Training\")\n",
    "def fsi_fraud_pipeline(num_samples: int = 10000):\n",
    "    \"\"\"Fraud detection training pipeline (validate step added via Elyra).\"\"\"\n",
    "    data_task = data_processing(num_samples=num_samples)\n",
    "    feature_task = feature_extract(dataset=data_task.outputs[\"dataset\"])\n",
    "    train_task = train_model(features=feature_task.outputs[\"features\"])\n",
    "    upload_model(model=train_task.outputs[\"model\"])\n",
    "\n",
    "\n",
    "output_file = \"fsi-fraud-pipeline.yaml\"\n",
    "compiler.Compiler().compile(fsi_fraud_pipeline, output_file)\n",
    "print(f\"Pipeline compiled to: {output_file}\")\n",
    "print(\"Import via: RHOAI Dashboard -> Pipelines -> Import pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
