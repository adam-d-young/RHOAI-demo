{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test: Query the Deployed Model\n",
    "\n",
    "This notebook sends prediction requests to our TensorFlow model running on **NVIDIA Triton Inference Server**.\n",
    "\n",
    "**What's happening under the hood:**\n",
    "- The model was trained in `train-and-upload.py` and uploaded to MinIO (S3)\n",
    "- KServe pulled the model from S3 into a Triton pod\n",
    "- Triton loaded the SavedModel onto the **GPU** and is serving it via REST API\n",
    "- We're calling it from inside the cluster using Triton's [v2 inference protocol](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/inference_protocols.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Discover the InferenceService & Check Model Health\n\nFirst, auto-detect the InferenceService name — the RHOAI Dashboard generates the name from the model registry entry, so we can't hardcode it. Then query Triton's metadata endpoint to discover the input/output tensor names.\n\n> **Why auto-detect tensors?** TF/Keras appends a numeric suffix to tensor names each time you export (`keras_tensor`, `keras_tensor_9`, `keras_tensor_12`, etc.). Hardcoding the name would break on re-export."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\nimport subprocess\n\n# --- Auto-detect InferenceService name ---\n# Try oc/kubectl first (available in RHOAI workbench images)\ntry:\n    result = subprocess.run(\n        [\"oc\", \"get\", \"inferenceservice\", \"-o\", \"jsonpath={.items[0].metadata.name}\"],\n        capture_output=True, text=True, timeout=10\n    )\n    if result.returncode == 0 and result.stdout.strip():\n        isvc_name = result.stdout.strip()\n    else:\n        raise RuntimeError(result.stderr)\nexcept Exception as e:\n    print(f\"oc lookup failed ({e}), trying k8s API...\")\n    NAMESPACE = open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\").read().strip()\n    TOKEN = open(\"/var/run/secrets/kubernetes.io/serviceaccount/token\").read().strip()\n    k8s_url = f\"https://kubernetes.default.svc/apis/serving.kserve.io/v1beta1/namespaces/{NAMESPACE}/inferenceservices\"\n    resp = requests.get(k8s_url, headers={\"Authorization\": f\"Bearer {TOKEN}\"}, verify=\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n    isvc_list = resp.json()\n    if \"items\" not in isvc_list:\n        print(f\"API error: {json.dumps(isvc_list, indent=2)}\")\n        raise\n    isvc_name = isvc_list[\"items\"][0][\"metadata\"][\"name\"]\n\nNAMESPACE = open(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\").read().strip()\nprint(f\"Found InferenceService: {isvc_name}\")\n\n# --- Build Triton URL ---\nMODEL_NAME = \"demo-model\"\nTRITON_URL = f\"http://{isvc_name}-predictor.{NAMESPACE}.svc.cluster.local:8080/v2/models/{MODEL_NAME}\"\nprint(f\"Triton endpoint:       {TRITON_URL}\")\n\n# --- Query model metadata ---\nresp = requests.get(TRITON_URL)\nmetadata = resp.json()\n\ninput_name = metadata['inputs'][0]['name']\ninput_shape = metadata['inputs'][0]['shape']\noutput_name = metadata['outputs'][0]['name']\n\nprint(f\"\\nModel:    {metadata['name']}\")\nprint(f\"Version:  {metadata['versions'][0]}\")\nprint(f\"Platform: {metadata['platform']}\")\nprint(f\"Input:    {input_name} shape={input_shape}\")\nprint(f\"Output:   {output_name} shape={metadata['outputs'][0]['shape']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Send a Prediction\n",
    "\n",
    "Our model takes **5 float features** and returns a **sigmoid probability** (0 to 1).\n",
    "\n",
    "The request format follows Triton's v2 protocol:\n",
    "- `name`: the input tensor name (auto-detected above)\n",
    "- `shape`: `[1, 5]` — one sample with 5 features\n",
    "- `datatype`: `FP32` — 32-bit floating point\n",
    "- `data`: the actual feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [0.1, 0.5, 0.3, 0.7, 0.2]\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [{\n",
    "        \"name\": input_name,\n",
    "        \"shape\": [1, 5],\n",
    "        \"datatype\": \"FP32\",\n",
    "        \"data\": data_1\n",
    "    }]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{TRITON_URL}/infer\", json=payload)\n",
    "result = resp.json()\n",
    "prediction_1 = result['outputs'][0]['data'][0]\n",
    "\n",
    "print(f\"Input:      {data_1}\")\n",
    "print(f\"Prediction: {prediction_1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try Different Inputs\n",
    "\n",
    "Changing the feature values produces a different prediction — proof that the model is actually computing, not returning a static value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = [0.9, 0.1, 0.8, 0.2, 0.95]\n",
    "\n",
    "payload[\"inputs\"][0][\"data\"] = data_2\n",
    "resp = requests.post(f\"{TRITON_URL}/infer\", json=payload)\n",
    "result = resp.json()\n",
    "prediction_2 = result['outputs'][0]['data'][0]\n",
    "\n",
    "print(f\"Input:      {data_2}\")\n",
    "print(f\"Prediction: {prediction_2:.6f}\")\n",
    "print(f\"\\nDifferent inputs → different predictions ({prediction_1:.4f} vs {prediction_2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened\n",
    "\n",
    "1. Python `requests` sent JSON over HTTP to the **Triton pod** inside the cluster\n",
    "2. Triton ran the **forward pass** through our neural network on the **A10G GPU**\n",
    "3. Result: a sigmoid probability between 0 and 1\n",
    "\n",
    "**In production, this same pattern powers:**\n",
    "- Fraud detection scores on transactions\n",
    "- Credit risk assessments\n",
    "- Real-time pricing models\n",
    "\n",
    "The only differences would be a real trained model, external access via Gateway/HTTPRoute, and token authentication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}