{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test: Query the Deployed Model\n",
    "\n",
    "This notebook sends prediction requests to our TensorFlow model running on **NVIDIA Triton Inference Server**.\n",
    "\n",
    "**What's happening under the hood:**\n",
    "- The model was trained in `train-and-upload.py` and uploaded to MinIO (S3)\n",
    "- KServe pulled the model from S3 into a Triton pod\n",
    "- Triton loaded the SavedModel onto the **GPU** and is serving it via REST API\n",
    "- We're calling it from inside the cluster using Triton's [v2 inference protocol](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/inference_protocols.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check Model Health\n",
    "\n",
    "Query the model metadata endpoint to confirm Triton has it loaded and to discover the input/output tensor names.\n",
    "\n",
    "> **Why auto-detect?** TF/Keras appends a numeric suffix to tensor names each time you export (`keras_tensor`, `keras_tensor_9`, `keras_tensor_12`, etc.). Hardcoding the name would break on re-export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "MODEL_NAME = \"demo-model\"\n",
    "TRITON_URL = f\"http://fsi-demo-model-predictor.fsi-demo.svc.cluster.local:80/v2/models/{MODEL_NAME}\"\n",
    "\n",
    "resp = requests.get(TRITON_URL)\n",
    "metadata = resp.json()\n",
    "\n",
    "input_name = metadata['inputs'][0]['name']\n",
    "input_shape = metadata['inputs'][0]['shape']\n",
    "output_name = metadata['outputs'][0]['name']\n",
    "\n",
    "print(f\"Model:    {metadata['name']}\")\n",
    "print(f\"Version:  {metadata['versions'][0]}\")\n",
    "print(f\"Platform: {metadata['platform']}\")\n",
    "print(f\"Input:    {input_name} shape={input_shape}\")\n",
    "print(f\"Output:   {output_name} shape={metadata['outputs'][0]['shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Send a Prediction\n",
    "\n",
    "Our model takes **5 float features** and returns a **sigmoid probability** (0 to 1).\n",
    "\n",
    "The request format follows Triton's v2 protocol:\n",
    "- `name`: the input tensor name (auto-detected above)\n",
    "- `shape`: `[1, 5]` — one sample with 5 features\n",
    "- `datatype`: `FP32` — 32-bit floating point\n",
    "- `data`: the actual feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [0.1, 0.5, 0.3, 0.7, 0.2]\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [{\n",
    "        \"name\": input_name,\n",
    "        \"shape\": [1, 5],\n",
    "        \"datatype\": \"FP32\",\n",
    "        \"data\": data_1\n",
    "    }]\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{TRITON_URL}/infer\", json=payload)\n",
    "result = resp.json()\n",
    "prediction_1 = result['outputs'][0]['data'][0]\n",
    "\n",
    "print(f\"Input:      {data_1}\")\n",
    "print(f\"Prediction: {prediction_1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try Different Inputs\n",
    "\n",
    "Changing the feature values produces a different prediction — proof that the model is actually computing, not returning a static value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = [0.9, 0.1, 0.8, 0.2, 0.95]\n",
    "\n",
    "payload[\"inputs\"][0][\"data\"] = data_2\n",
    "resp = requests.post(f\"{TRITON_URL}/infer\", json=payload)\n",
    "result = resp.json()\n",
    "prediction_2 = result['outputs'][0]['data'][0]\n",
    "\n",
    "print(f\"Input:      {data_2}\")\n",
    "print(f\"Prediction: {prediction_2:.6f}\")\n",
    "print(f\"\\nDifferent inputs → different predictions ({prediction_1:.4f} vs {prediction_2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened\n",
    "\n",
    "1. Python `requests` sent JSON over HTTP to the **Triton pod** inside the cluster\n",
    "2. Triton ran the **forward pass** through our neural network on the **A10G GPU**\n",
    "3. Result: a sigmoid probability between 0 and 1\n",
    "\n",
    "**In production, this same pattern powers:**\n",
    "- Fraud detection scores on transactions\n",
    "- Credit risk assessments\n",
    "- Real-time pricing models\n",
    "\n",
    "The only differences would be a real trained model, external access via Gateway/HTTPRoute, and token authentication."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
